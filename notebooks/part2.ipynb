{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Structure\n",
    "```\n",
    "2DIP_exercise/\n",
    "│-- data/             # Contains images & videos\n",
    "│   │-- input/        # 1 image and 1 video for each phase respectively\n",
    "│   │-- output/       # All output images/videos must be stored here\n",
    "│-- notebooks/        # Jupyter Notebooks for each phase\n",
    "│   │-- part1.ipynb   # Image processing & feature extraction\n",
    "│   │-- part2.ipynb   # Optical flow, object detection and tracking \n",
    "│-- README.md         # Project instructions\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths\n",
    "base_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "inputs = os.path.join(base_path, 'data','input')\n",
    "outputs = os.path.join(base_path, 'data','output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary Code for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(image):\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(image_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames(video_path):\n",
    "    # Re-open the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    frames = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Convert BGR to RGB for matplotlib\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame_rgb)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['animation.embed_limit'] = 100\n",
    "\n",
    "def display_video(video_path):\n",
    "    \n",
    "    frames = get_frames(video_path)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "    im = ax.imshow(np.zeros_like(frames[0]))\n",
    "    ax.axis('off')\n",
    "\n",
    "    def update(frame):\n",
    "        im.set_array(frame)\n",
    "        return [im]\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, update, frames=frames, interval=50, blit=True, repeat=False)\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "    return ani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 : Analyze movement patterns in a video sequence. **(6)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Compute dense optical flow for each frame in a video of a moving crowd. **(2)**\n",
    "\n",
    "b) Visualize the movement patterns in 2 different ways. **(2+2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hsv_color_coded_visualization(flow, frame):\n",
    "    \"\"\"\n",
    "    Visualizes optical flow using HSV color coding. This method visualizes optical flow using the HSV color space, \n",
    "    where hue represents the direction of flow and value represents the magnitude.\n",
    "    Args:\n",
    "        flow: Optical flow array of shape (height, width, 2).\n",
    "        frame: The original frame to overlay the flow visualization.\n",
    "    Returns:\n",
    "        color_vis: Color-coded optical flow visualization.\n",
    "    \"\"\"\n",
    "    # get a 2-channel array with optical flow vectors (u,v) and calculate magnitude and angle\n",
    "    magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1]) # angle in radians\n",
    "    hsv = np.zeros_like(frame)\n",
    "    hsv[..., 1] = 255\n",
    "    hsv[..., 0] = angle * 180 / np.pi / 2 # convert radians to degrees and scale to [0, 180] for hue\n",
    "    hsv[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    flow_color = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "    color_vis = cv2.addWeighted(frame, 0.6, flow_color, 0.4, 0)\n",
    "\n",
    "    return flow_color, color_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrow_visualization(flow, frame, step=16):\n",
    "    \"\"\"\n",
    "    Visualizes optical flow using arrows. This method draws arrows on the original frame to represent the flow vectors.\n",
    "    Args:\n",
    "        flow: Optical flow array of shape (height, width, 2).\n",
    "        frame: The original frame to overlay the flow visualization.\n",
    "        step: Step size for arrow placement.\n",
    "    Returns:\n",
    "        arrow_vis: Frame with arrows representing optical flow.\n",
    "    \"\"\"\n",
    "    h, w = flow.shape[:2]\n",
    "    y, x = np.mgrid[step // 2 : h : step, step // 2 : w : step]\n",
    "    arrow_vis = frame.copy()\n",
    "    for yi, xi in zip(y.flatten(), x.flatten()):\n",
    "        fx, fy = flow[yi, xi]\n",
    "        pt1 = (int(xi), int(yi))\n",
    "        pt2 = (int(xi + fx), int(yi + fy))\n",
    "\n",
    "        cv2.arrowedLine(\n",
    "            img=arrow_vis,\n",
    "            pt1=pt1,\n",
    "            pt2=pt2,\n",
    "            color=(0, 255, 0),\n",
    "            thickness=1,\n",
    "            tipLength=0.3,\n",
    "        )\n",
    "    return arrow_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optical_flow(video_path, output_path1, output_path2, output_path3):\n",
    "    \"\"\"\n",
    "    Compute and visualize dense optical flow for a video, saving both visualizations.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Path to input video file\n",
    "        output_path1 (str): Path to save color-coded flow visualization\n",
    "        output_path2 (str): Path to save arrow visualization\n",
    "        output_path3 (str): Path to save flow color visualization\n",
    "    \"\"\"\n",
    "    # Open video capture\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # Create VideoWriter objects\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out1 = cv2.VideoWriter(output_path1, fourcc, fps, (frame_width, frame_height))\n",
    "    out2 = cv2.VideoWriter(output_path2, fourcc, fps, (frame_width, frame_height))\n",
    "    out3 = cv2.VideoWriter(output_path3, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    # Read first frame\n",
    "    ret, old_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error reading video\")\n",
    "        return\n",
    "\n",
    "    # Convert to grayscale\n",
    "    old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    while True:\n",
    "        # Read next frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert to grayscale\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Calculate optical flow\n",
    "        flow = cv2.calcOpticalFlowFarneback(\n",
    "            old_gray,\n",
    "            frame_gray,\n",
    "            None,\n",
    "            pyr_scale=0.5,\n",
    "            levels=5,\n",
    "            winsize=15,\n",
    "            iterations=5,\n",
    "            poly_n=5,\n",
    "            poly_sigma=1.2,\n",
    "            flags=0,\n",
    "        )\n",
    "\n",
    "        # Generate hsv color-coded visualization\n",
    "        flow_color, color_vis = hsv_color_coded_visualization(flow, frame)\n",
    "\n",
    "        # Generate arrow visualization\n",
    "        arrow_vis = arrow_visualization(flow, frame)\n",
    "\n",
    "        # Write frames to output videos\n",
    "        out1.write(color_vis)\n",
    "        out2.write(arrow_vis)\n",
    "        out3.write(flow_color)\n",
    "\n",
    "        # Update previous frame\n",
    "        old_gray = frame_gray\n",
    "\n",
    "    # Release everything\n",
    "    cap.release()\n",
    "    out1.release()\n",
    "    out2.release()\n",
    "    out3.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = os.path.join(inputs, 'part2.mp4')  # Replace with your input video path\n",
    "output_path1 = os.path.join(outputs, 'optical_flow_1.mp4')  # Output visualization video path\n",
    "output_path2 = os.path.join(outputs, 'optical_flow_2.mp4')  # Output visualization video path\n",
    "output_path3 = os.path.join(outputs, 'optical_flow_3.mp4')  # Output visualization video path\n",
    "\n",
    "optical_flow(video_path, output_path1, output_path2, output_path3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = display_video(output_path1)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = display_video(output_path2)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = display_video(output_path3)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 : Identify and track a moving object in a video sequence. **(9)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Detect an object using template matching. The output would be the first frame where it appears, with a bounding box around the detected object. **(2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_object(video_path, template_path, output_path):\n",
    "    \"\"\"\n",
    "    Detect first occurrence of a template object in a video using template matching.\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to input video file\n",
    "        template_path (str): Path to template image file\n",
    "        output_path (str): Path to save output frame with detection\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (frame_number, location) if object found, else None\n",
    "    \"\"\"\n",
    "    # Read template\n",
    "    template = cv2.imread(template_path)\n",
    "    template_gray = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)\n",
    "    w, h = template_gray.shape[::-1]\n",
    "    \n",
    "    # Open video capture\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    \n",
    "    # Template matching threshold\n",
    "    threshold = 0.8\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        frame_count += 1\n",
    "        \n",
    "        # Convert frame to grayscale\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Perform template matching\n",
    "        result = cv2.matchTemplate(frame_gray, template_gray, cv2.TM_CCOEFF_NORMED)\n",
    "        \n",
    "        # Find locations where matching exceeds threshold\n",
    "        locations = np.where(result >= threshold)\n",
    "        \n",
    "        # Check if object is found\n",
    "        if locations[0].size > 0:\n",
    "            # Get the best match location\n",
    "            min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
    "            top_left = max_loc\n",
    "            bottom_right = (top_left[0] + w, top_left[1] + h)\n",
    "            \n",
    "            # Draw rectangle around detected object\n",
    "            cv2.rectangle(frame, top_left, bottom_right, (0, 255, 0), 2)\n",
    "            \n",
    "            # Add text showing frame number\n",
    "            text = f\"Frame: {frame_count}\"\n",
    "            cv2.putText(frame, text, (10, 30), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            \n",
    "            # Save the frame with detection\n",
    "            cv2.imwrite(output_path, frame)\n",
    "            \n",
    "            cap.release()\n",
    "            return frame\n",
    "    \n",
    "    cap.release()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = os.path.join(inputs, 'part2.mp4')  # Replace with your input video path\n",
    "template_path = os.path.join(inputs, 'template.png')  # Replace with your template image path\n",
    "output_path = os.path.join(outputs, 'detected_object.jpg')  # Output video path\n",
    "\n",
    "image = locate_object(video_path, template_path, output_path)\n",
    "display_images(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Implement a Kalman filter to predict the object's position in subsequent frames. **(5)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track(video_path, template_path, output_path):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = os.path.join(inputs, 'part2.mp4')  # Replace with your input video path\n",
    "template_path = os.path.join(inputs, 'template.png')  # Replace with your template image path\n",
    "output_path = os.path.join(outputs, 'tracked_object.mp4')  # Output video path\n",
    "\n",
    "track(video_path, template_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = display_video(output_path)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Compare Bayesian filtering and Kalman filtering (theoretically). **(2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO c):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
